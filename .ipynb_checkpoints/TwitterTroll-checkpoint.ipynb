{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Project for COGS118A, this is meant to find and identify russian 'troll' tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members :\n",
    "- Gael Van der Lee\n",
    "- Alex Labranche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 3 datasets\n",
    "troll_users = pd.read_csv('Tweets/Trolls/users.csv')\n",
    "troll_tweets = pd.read_csv('Tweets/Trolls/tweets.csv')\n",
    "normal_tweets = pd.read_csv('Tweets/Normal/dashboard_x_usa_x_filter_nativeretweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns of each dataframe at import :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Nicknames consistent\n",
    "troll_users['screen_name'] = troll_users['screen_name'].str.lower()\n",
    "\n",
    "# Drop the irrelevant data\n",
    "users_to_drop = ['statuses_count', 'time_zone', 'verified', 'favourites_count']\n",
    "tweets_to_drop = ['created_at', 'tweet_id', 'source', 'posted', 'retweeted_status_id', 'retweeted', 'in_reply_to_status_id']\n",
    "normal_to_drop = ['Tweet Id', 'Latitude', 'Longitude', 'Country', 'Profile picture', 'Tweet Url']\n",
    "troll_users = troll_users.drop(users_to_drop, axis=1)\n",
    "troll_tweets = troll_tweets.drop(tweets_to_drop, axis=1)\n",
    "normal_tweets = normal_tweets.drop(normal_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_new_cols = {'id': 'User Id', 'location': 'Location', 'name': 'User Name', \n",
    "                  'followers_count': 'Followers', 'lang': 'Language', 'screen_name': 'Nickname',\n",
    "                  'description': 'Bio', 'created_at': 'Account creation date', \n",
    "                  'friends_count': 'Following', 'listed_count': 'Listed'}\n",
    "tweets_new_cols = {'user_id': 'User Id', 'user_key': 'Nickname', 'created_str': 'Tweet date',\n",
    "                   'retweet_count': 'Retweets', 'favorite_count': 'Favorites', 'text': 'Tweet',\n",
    "                   'hashtags': 'Hashtags', 'expanded_urls': 'URLs', 'mentions': 'Mentions'}\n",
    "normal_new_cols = {'Tweet content': 'Tweet', 'Favs': 'Favorites', 'RTs': 'Retweets', \n",
    "                   'Place (as appears on Bio)': 'Location', 'Tweet language (ISO 639-1)': 'Language'}\n",
    "\n",
    "troll_users = troll_users.rename(columns=users_new_cols)\n",
    "troll_tweets = troll_tweets.rename(columns=tweets_new_cols)\n",
    "normal_tweets = normal_tweets.rename(columns=normal_new_cols)\n",
    "\n",
    "# Make times formats the same\n",
    "normal_tweets['Tweet date'] = normal_tweets['Date'] + ' ' + normal_tweets['Hour']\n",
    "normal_tweets = normal_tweets.drop(['Date', 'Hour'], axis=1)\n",
    "\n",
    "# Helps with merging\n",
    "troll_users = troll_users.drop(['User Id', 'Account creation date'], axis=1)\n",
    "troll_tweets = troll_tweets.drop('User Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Columns of each dataframe after cleaning :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling and merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the troll datasets\n",
    "trolls = pd.merge(left=troll_tweets, right=troll_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trolls.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tweets.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put labels\n",
    "trolls['y'] = pd.Series([1 for x in range(len(trolls))], index=trolls.index)\n",
    "normal_tweets['y'] = pd.Series([0 for x in range(len(normal_tweets))], index=normal_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge normal and troll data \n",
    "tweets = trolls.append(normal_tweets, ignore_index=True)\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random rows we will use for visualization\n",
    "rows = [random.randint(0, len(tweets)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_hashtag(string):\n",
    "    htags = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '#':\n",
    "            htags.append(word[1:])\n",
    "    splits= []\n",
    "    # Puts space before capital letters and numbers\n",
    "    for h in htags:\n",
    "        splits.append(re.sub(r\"([A-Z0-9])\", r\" \\1\", h)[1:])\n",
    "    return splits\n",
    "\n",
    "def extract_mentions(string):\n",
    "    men = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '@':\n",
    "            if word[-1] == ':':\n",
    "                men.append(word[1:-1])\n",
    "            else:\n",
    "                men.append(word[1:])\n",
    "    return men\n",
    "\n",
    "# Not implemented yet, takes way too long to run on ~400,000 tweets\n",
    "def extract_links(string):\n",
    "    links = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[:4] == 'http':\n",
    "            try:\n",
    "                link = urlopen(word).geturl()\n",
    "                links.append(link)\n",
    "            except:\n",
    "                pass\n",
    "    return links\n",
    "\n",
    "def extract_website(string):\n",
    "    string = str(string)\n",
    "    start = string.find('//') + 2\n",
    "    if start > 1:\n",
    "        end = string[start:].find('/') + start\n",
    "        return string[start:end]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tweets['URLs'].apply(extract_website)\n",
    "test[test != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolls[(trolls['test'].str.len() != 0)][['Tweet', 'URLs', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Hashtags'] = tweets['Tweet'].apply(extract_hashtag)\n",
    "tweets['Mentions'] = tweets['Tweet'].apply(extract_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    string = str(string)\n",
    "    string = string.split()\n",
    "    to_remove = []\n",
    "    for word in string:\n",
    "        if word[0] == '#' or word[0] == '@' or word == 'RT':\n",
    "            to_remove.append(word)\n",
    "    for word in to_remove:\n",
    "        string.remove(word)\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Tweet'] = tweets['Tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace NaNs with 0s where appropriate\n",
    "#replace NaNs with empty list where appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for NLP : One hot encoding and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary data reformatting\n",
    "#replace language with one-hot encoded form\n",
    "#reformat date (?)\n",
    "#probably won't use: bio, listed, location, nickname, username, URLs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove newlines for one-hot encoding\n",
    "tweets_1_hot = tweets.copy()\n",
    "tweets_1_hot['Tweet'].str.replace('\\n', ' ', case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create 1 hot dataframe :\n",
    "https://stackoverflow.com/questions/18889588/create-dummies-from-column-with-multiple-values-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create word2vec dataframe\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for classification : SVM and K Nearsest Neighbors \n",
    "\n",
    "NOTE: The gridsearch objects may require parameters to reduce memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create SVM\n",
    "#initialize classifier objects for the 1-hot and word2vec encodings\n",
    "svc_1_hot = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "svc_word2vec = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#penalty parameter\n",
    "C_list =[10**(1-i) for i in range(6)]\n",
    "#kernel function - poly:polynomial\n",
    "kernel_list =['linear','rbf','sigmoid','poly']\n",
    "param_dic_SVM = {'C':C_list, 'kernel':kernel_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_svm_1 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_svm_2 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create KNN\n",
    "\n",
    "knn_one_hot = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knn_word2vec = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#number of nearest neighbors to consider: 3-10\n",
    "n_neighbors_list = [i+3 for i in range(8)]\n",
    "#weighting of neighbors, whether they all get an equal vote or are weighted by distance\n",
    "weights_list = ['uniform','distance']\n",
    "param_dic_knn = {'n_neighbors':n_neighbors_list,'weights':weights_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_knn_1 = GridSearchCV(knn_one_hot, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_knn_2 = GridSearchCV(knn_word2vec, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plot and present results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
