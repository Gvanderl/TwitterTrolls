{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Project for COGS118A, this is meant to find and identify russian 'troll' tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members :\n",
    "- Gael Van der Lee\n",
    "- Alex Labranche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the 3 datasets\n",
    "troll_users = pd.read_csv('Tweets/Trolls/users.csv')\n",
    "troll_tweets = pd.read_csv('Tweets/Trolls/tweets.csv')\n",
    "normal_tweets = pd.read_csv('Tweets/Normal/dashboard_x_usa_x_filter_nativeretweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of each dataframe at import :\n",
      "\n",
      "Troll users : ['id' 'location' 'name' 'followers_count' 'statuses_count' 'time_zone'\n",
      " 'verified' 'lang' 'screen_name' 'description' 'created_at'\n",
      " 'favourites_count' 'friends_count' 'listed_count']\n",
      "\n",
      "Troll tweets : ['user_id' 'user_key' 'created_at' 'created_str' 'retweet_count'\n",
      " 'retweeted' 'favorite_count' 'text' 'tweet_id' 'source' 'hashtags'\n",
      " 'expanded_urls' 'posted' 'mentions' 'retweeted_status_id'\n",
      " 'in_reply_to_status_id']\n",
      "\n",
      "Normal tweets : ['Tweet Id' 'Date' 'Hour' 'User Name' 'Nickname' 'Bio' 'Tweet content'\n",
      " 'Favs' 'RTs' 'Latitude' 'Longitude' 'Country' 'Place (as appears on Bio)'\n",
      " 'Profile picture' 'Followers' 'Following' 'Listed'\n",
      " 'Tweet language (ISO 639-1)' 'Tweet Url']\n"
     ]
    }
   ],
   "source": [
    "print('Columns of each dataframe at import :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Nicknames consistent\n",
    "troll_users['screen_name'] = troll_users['screen_name'].str.lower()\n",
    "\n",
    "# Drop the irrelevant data\n",
    "users_to_drop = ['statuses_count', 'time_zone', 'verified', 'favourites_count']\n",
    "tweets_to_drop = ['created_at', 'tweet_id', 'source', 'posted', 'retweeted_status_id', 'retweeted', 'in_reply_to_status_id']\n",
    "normal_to_drop = ['Tweet Id', 'Latitude', 'Longitude', 'Country', 'Profile picture', 'Tweet Url']\n",
    "troll_users = troll_users.drop(users_to_drop, axis=1)\n",
    "troll_tweets = troll_tweets.drop(tweets_to_drop, axis=1)\n",
    "normal_tweets = normal_tweets.drop(normal_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_new_cols = {'id': 'User Id', 'location': 'Location', 'name': 'User Name', \n",
    "                  'followers_count': 'Followers', 'lang': 'Language', 'screen_name': 'Nickname',\n",
    "                  'description': 'Bio', 'created_at': 'Account creation date', \n",
    "                  'friends_count': 'Following', 'listed_count': 'Listed'}\n",
    "tweets_new_cols = {'user_id': 'User Id', 'user_key': 'Nickname', 'created_str': 'Tweet date',\n",
    "                   'retweet_count': 'Retweets', 'favorite_count': 'Favorites', 'text': 'Tweet',\n",
    "                   'hashtags': 'Hashtags', 'expanded_urls': 'URLs', 'mentions': 'Mentions'}\n",
    "normal_new_cols = {'Tweet content': 'Tweet', 'Favs': 'Favorites', 'RTs': 'Retweets', \n",
    "                   'Place (as appears on Bio)': 'Location', 'Tweet language (ISO 639-1)': 'Language'}\n",
    "\n",
    "troll_users = troll_users.rename(columns=users_new_cols)\n",
    "troll_tweets = troll_tweets.rename(columns=tweets_new_cols)\n",
    "normal_tweets = normal_tweets.rename(columns=normal_new_cols)\n",
    "\n",
    "# Make times formats the same\n",
    "normal_tweets['Tweet date'] = normal_tweets['Date'] + ' ' + normal_tweets['Hour']\n",
    "normal_tweets = normal_tweets.drop(['Date', 'Hour'], axis=1)\n",
    "\n",
    "# Helps with merging\n",
    "troll_users = troll_users.drop(['User Id', 'Account creation date'], axis=1)\n",
    "troll_tweets = troll_tweets.drop('User Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of each dataframe after cleaning :\n",
      "\n",
      "Troll users : ['Location' 'User Name' 'Followers' 'Language' 'Nickname' 'Bio'\n",
      " 'Following' 'Listed']\n",
      "\n",
      "Troll tweets : ['Nickname' 'Tweet date' 'Retweets' 'Favorites' 'Tweet' 'Hashtags' 'URLs'\n",
      " 'Mentions']\n",
      "\n",
      "Normal tweets : ['User Name' 'Nickname' 'Bio' 'Tweet' 'Favorites' 'Retweets' 'Location'\n",
      " 'Followers' 'Following' 'Listed' 'Language' 'Tweet date']\n"
     ]
    }
   ],
   "source": [
    "print('Columns of each dataframe after cleaning :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling and merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the troll datasets\n",
    "trolls = pd.merge(left=troll_tweets, right=troll_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Tweet date</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>URLs</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Location</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Language</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Following</th>\n",
       "      <th>Listed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kathiemrr</td>\n",
       "      <td>2017-02-27 14:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#ThingsDoneByMistake kissing auntie in the lips</td>\n",
       "      <td>[\"ThingsDoneByMistake\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Imperfection is beauty, madness is genius and ...</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kathiemrr</td>\n",
       "      <td>2016-11-28 16:17:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jadedsweetangel: #ToDoListBeforeChristmas ...</td>\n",
       "      <td>[\"ToDoListBeforeChristmas\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Imperfection is beauty, madness is genius and ...</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Nickname           Tweet date  Retweets  Favorites  \\\n",
       "0  kathiemrr  2017-02-27 14:54:00       NaN        NaN   \n",
       "1  kathiemrr  2016-11-28 16:17:36       NaN        NaN   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0    #ThingsDoneByMistake kissing auntie in the lips   \n",
       "1  RT @jadedsweetangel: #ToDoListBeforeChristmas ...   \n",
       "\n",
       "                      Hashtags URLs Mentions Location User Name  Followers  \\\n",
       "0      [\"ThingsDoneByMistake\"]   []       []  Atlanta    Kathie     2970.0   \n",
       "1  [\"ToDoListBeforeChristmas\"]   []       []  Atlanta    Kathie     2970.0   \n",
       "\n",
       "  Language                                                Bio  Following  \\\n",
       "0       en  Imperfection is beauty, madness is genius and ...     3157.0   \n",
       "1       en  Imperfection is beauty, madness is genius and ...     3157.0   \n",
       "\n",
       "   Listed  \n",
       "0    22.0  \n",
       "1    22.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trolls.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Name</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Location</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Listed</th>\n",
       "      <th>Language</th>\n",
       "      <th>Tweet date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Schulhoff</td>\n",
       "      <td>BillSchulhoff</td>\n",
       "      <td>Husband,Dad,GrandDad,Ordained Minister, Umpire...</td>\n",
       "      <td>Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East Patchogue, NY</td>\n",
       "      <td>386.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>en</td>\n",
       "      <td>2016-04-16 12:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniele Polis</td>\n",
       "      <td>danipolis</td>\n",
       "      <td>Viagens, geek, moda, batons laranja, cabelos c...</td>\n",
       "      <td>Pausa pro café antes de embarcar no próximo vô...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grapevine, TX</td>\n",
       "      <td>812.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>pt</td>\n",
       "      <td>2016-04-16 12:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User Name       Nickname  \\\n",
       "0  Bill Schulhoff  BillSchulhoff   \n",
       "1   Daniele Polis      danipolis   \n",
       "\n",
       "                                                 Bio  \\\n",
       "0  Husband,Dad,GrandDad,Ordained Minister, Umpire...   \n",
       "1  Viagens, geek, moda, batons laranja, cabelos c...   \n",
       "\n",
       "                                               Tweet  Favorites  Retweets  \\\n",
       "0  Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...        NaN       NaN   \n",
       "1  Pausa pro café antes de embarcar no próximo vô...        NaN       NaN   \n",
       "\n",
       "             Location  Followers  Following  Listed Language        Tweet date  \n",
       "0  East Patchogue, NY      386.0      705.0    24.0       en  2016-04-16 12:44  \n",
       "1       Grapevine, TX      812.0      647.0    16.0       pt  2016-04-16 12:44  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_tweets.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put labels\n",
    "trolls['y'] = pd.Series([1 for x in range(len(trolls))], index=trolls.index)\n",
    "normal_tweets['y'] = pd.Series([0 for x in range(len(normal_tweets))], index=normal_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408271, 16)\n"
     ]
    }
   ],
   "source": [
    "# Merge normal and troll data \n",
    "tweets = trolls.append(normal_tweets, ignore_index=True)\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random rows we will use for visualization\n",
    "rows = [random.randint(0, len(tweets)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bio</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Language</th>\n",
       "      <th>Listed</th>\n",
       "      <th>Location</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet date</th>\n",
       "      <th>URLs</th>\n",
       "      <th>User Name</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77808</th>\n",
       "      <td>if you don't work hard to achieve your dreams,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24344.0</td>\n",
       "      <td>21953.0</td>\n",
       "      <td>[\"IdRunForPresidentIf\"]</td>\n",
       "      <td>en</td>\n",
       "      <td>113.0</td>\n",
       "      <td>Pittsburgh, US</td>\n",
       "      <td>[]</td>\n",
       "      <td>giselleevns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @HapennyHoHum: #IdRunForPresidentIf I could...</td>\n",
       "      <td>2016-12-07 15:20:57</td>\n",
       "      <td>[]</td>\n",
       "      <td>Giselle Evans</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261291</th>\n",
       "      <td>Malandrin Nation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hanford, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Malandrines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Making a pizza @ Fatte Alberts Pizza Co. https...</td>\n",
       "      <td>2016-04-16 00:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Malandrines</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286121</th>\n",
       "      <td>Follow this account for geo-targeted Cosmetolo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Chester Heights, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tmj_phl_cosmo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Cosmetology #Job in #GlenMills, PA: Hair Styl...</td>\n",
       "      <td>2016-04-15 20:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TMJ-PHL Cosmo Jobs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322678</th>\n",
       "      <td>Seven continent marathoner (55 and counting). ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>423.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paulbkny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hanging out with my friend Bruce. #sharks #fam...</td>\n",
       "      <td>2016-04-15 15:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paul Nelson</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Bio  Favorites  \\\n",
       "77808   if you don't work hard to achieve your dreams,...        NaN   \n",
       "261291                                   Malandrin Nation        NaN   \n",
       "286121  Follow this account for geo-targeted Cosmetolo...        NaN   \n",
       "322678  Seven continent marathoner (55 and counting). ...        NaN   \n",
       "\n",
       "        Followers  Following                 Hashtags Language  Listed  \\\n",
       "77808     24344.0    21953.0  [\"IdRunForPresidentIf\"]       en   113.0   \n",
       "261291      437.0      156.0                      NaN       en     5.0   \n",
       "286121      235.0      225.0                      NaN       en     9.0   \n",
       "322678      423.0      582.0                      NaN       en     9.0   \n",
       "\n",
       "                   Location Mentions       Nickname  Retweets  \\\n",
       "77808        Pittsburgh, US       []    giselleevns       NaN   \n",
       "261291          Hanford, CA      NaN    Malandrines       NaN   \n",
       "286121  Chester Heights, PA      NaN  tmj_phl_cosmo       NaN   \n",
       "322678         Florida, USA      NaN       paulbkny       NaN   \n",
       "\n",
       "                                                    Tweet  \\\n",
       "77808   RT @HapennyHoHum: #IdRunForPresidentIf I could...   \n",
       "261291  Making a pizza @ Fatte Alberts Pizza Co. https...   \n",
       "286121  #Cosmetology #Job in #GlenMills, PA: Hair Styl...   \n",
       "322678  Hanging out with my friend Bruce. #sharks #fam...   \n",
       "\n",
       "                 Tweet date URLs           User Name  y  \n",
       "77808   2016-12-07 15:20:57   []       Giselle Evans  1  \n",
       "261291     2016-04-16 00:14  NaN     Los Malandrines  0  \n",
       "286121     2016-04-15 20:21  NaN  TMJ-PHL Cosmo Jobs  0  \n",
       "322678     2016-04-15 15:44  NaN         Paul Nelson  0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtag(string):\n",
    "    htags = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '#':\n",
    "            htags.append(word[1:])\n",
    "    splits= []\n",
    "    # Puts space before capital letters and numbers\n",
    "    for h in htags:\n",
    "        splits.append(re.sub(r\"([A-Z0-9])\", r\" \\1\", h)[1:])\n",
    "    return splits\n",
    "\n",
    "def extract_mentions(string):\n",
    "    men = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '@':\n",
    "            if word[-1] == ':':\n",
    "                men.append(word[1:-1])\n",
    "            else:\n",
    "                men.append(word[1:])\n",
    "    return men\n",
    "\n",
    "# Not implemented yet, takes way too long to run on ~400,000 tweets\n",
    "def extract_links(string):\n",
    "    links = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[:4] == 'http':\n",
    "            try:\n",
    "                link = urlopen(word).geturl()\n",
    "                links.append(link)\n",
    "            except:\n",
    "                pass\n",
    "    return links\n",
    "\n",
    "def extract_website(string):\n",
    "    string = str(string)\n",
    "    start = string.find('//') + 2\n",
    "    if start > 1:\n",
    "        end = string[start:].find('/') + start\n",
    "        return string[start:end]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330                  twitter.com\n",
       "655              www.newsmax.com\n",
       "696                wikileaks.org\n",
       "1306                 twitter.com\n",
       "1504                 twitter.com\n",
       "1574                    paper.li\n",
       "1621                     vine.co\n",
       "2002                 twitter.com\n",
       "2833                 twitter.com\n",
       "3340                 twitter.com\n",
       "3461                 twitter.com\n",
       "3758                       ln.is\n",
       "3874                       ln.is\n",
       "4028            proudemocrat.com\n",
       "4095                 twitter.com\n",
       "4675                 twitter.com\n",
       "5172           cards.twitter.com\n",
       "5204                   on.cc.com\n",
       "5634                 twitter.com\n",
       "5719                 twitter.com\n",
       "7333                 twitter.com\n",
       "7337             blacktolive.org\n",
       "7342                      bit.ly\n",
       "7343                      bit.ly\n",
       "7344                 twitter.com\n",
       "7345                    youtu.be\n",
       "7351                      bit.ly\n",
       "7353                      bit.ly\n",
       "7356                 twitter.com\n",
       "7357           www.lifezette.com\n",
       "                   ...          \n",
       "203347                to.welt.de\n",
       "203348    www.ruhrnachrichten.de\n",
       "203349                    goo.gl\n",
       "203350                    goo.gl\n",
       "203352               www.faz.net\n",
       "203353          www.kn-online.de\n",
       "203355               www.welt.de\n",
       "203364         www.tagesschau.de\n",
       "203369          www.politico.com\n",
       "203370               www.cnn.com\n",
       "203371          www.politico.com\n",
       "203372    www.huffingtonpost.com\n",
       "203373                 riafan.ru\n",
       "203380                    bit.ly\n",
       "203381                   vine.co\n",
       "203386              news.sky.com\n",
       "203387        www.wonderzine.com\n",
       "203394         www.tagesschau.de\n",
       "203400               metro.co.uk\n",
       "203404                   dlvr.it\n",
       "203405                   usat.ly\n",
       "203421                   vine.co\n",
       "203423                  youtu.be\n",
       "203424                  youtu.be\n",
       "203425                  youtu.be\n",
       "203430         www.georeview.org\n",
       "203431         www.tagesschau.de\n",
       "203433                   faz.net\n",
       "203441               twitter.com\n",
       "203448                to.welt.de\n",
       "Name: URLs, Length: 25314, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweets['URLs'].apply(extract_website)\n",
    "test[test != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f18a653eb238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrolls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrolls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'URLs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "trolls[(trolls['test'].str.len() != 0)][['Tweet', 'URLs', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Hashtags'] = tweets['Tweet'].apply(extract_hashtag)\n",
    "tweets['Mentions'] = tweets['Tweet'].apply(extract_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    string = str(string)\n",
    "    string = string.split()\n",
    "    to_remove = []\n",
    "    for word in string:\n",
    "        if word[0] == '#' or word[0] == '@' or word == 'RT':\n",
    "            to_remove.append(word)\n",
    "    for word in to_remove:\n",
    "        string.remove(word)\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Tweet'] = tweets['Tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for NLP : One hot encoding and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create 1 hot dataframe :\n",
    "https://stackoverflow.com/questions/18889588/create-dummies-from-column-with-multiple-values-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create word2vec dataframe\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for classification : SVM and K Nearsest Neighbors \n",
    "\n",
    "NOTE: The gridsearch objects may require parameters to reduce memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create SVM\n",
    "#initialize classifier objects for the 1-hot and word2vec encodings\n",
    "svc_1_hot = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "svc_word2vec = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#penalty parameter\n",
    "C_list =[10**(1-i) for i in range(6)]\n",
    "#kernel function - poly:polynomial\n",
    "kernel_list =['linear','rbf','sigmoid','poly']\n",
    "param_dic_SVM = {'C':C_list, 'kernel':kernel_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_svm_1 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_svm_2 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create KNN\n",
    "\n",
    "knn_one_hot = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knn_word2vec = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#number of nearest neighbors to consider: 3-10\n",
    "n_neighbors_list = [i+3 for i in range(8)]\n",
    "#weighting of neighbors, whether they all get an equal vote or are weighted by distance\n",
    "weights_list = ['uniform','distance']\n",
    "param_dic_knn = {'n_neighbors':n_neighbors_list,'weights':weights_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_knn_1 = GridSearchCV(knn_one_hot, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_knn_2 = GridSearchCV(knn_word2vec, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plot and present results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
