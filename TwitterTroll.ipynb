{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Project for COGS118A, this is meant to find and identify russian 'troll' tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group members :\n",
    "- Gael Van der Lee\n",
    "- Alex Labranche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "#from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 3 datasets\n",
    "troll_users = pd.read_csv('Tweets/Trolls/users.csv')\n",
    "troll_tweets = pd.read_csv('Tweets/Trolls/tweets.csv')\n",
    "normal_tweets = pd.read_csv('Tweets/Normal/dashboard_x_usa_x_filter_nativeretweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of each dataframe at import :\n",
      "\n",
      "Troll users : ['id' 'location' 'name' 'followers_count' 'statuses_count' 'time_zone'\n",
      " 'verified' 'lang' 'screen_name' 'description' 'created_at'\n",
      " 'favourites_count' 'friends_count' 'listed_count']\n",
      "\n",
      "Troll tweets : ['user_id' 'user_key' 'created_at' 'created_str' 'retweet_count'\n",
      " 'retweeted' 'favorite_count' 'text' 'tweet_id' 'source' 'hashtags'\n",
      " 'expanded_urls' 'posted' 'mentions' 'retweeted_status_id'\n",
      " 'in_reply_to_status_id']\n",
      "\n",
      "Normal tweets : ['Tweet Id' 'Date' 'Hour' 'User Name' 'Nickname' 'Bio' 'Tweet content'\n",
      " 'Favs' 'RTs' 'Latitude' 'Longitude' 'Country' 'Place (as appears on Bio)'\n",
      " 'Profile picture' 'Followers' 'Following' 'Listed'\n",
      " 'Tweet language (ISO 639-1)' 'Tweet Url']\n"
     ]
    }
   ],
   "source": [
    "print('Columns of each dataframe at import :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Nicknames consistent\n",
    "troll_users['screen_name'] = troll_users['screen_name'].str.lower()\n",
    "\n",
    "# Drop the irrelevant data\n",
    "users_to_drop = ['statuses_count', 'time_zone', 'verified', 'favourites_count']\n",
    "tweets_to_drop = ['created_at', 'tweet_id', 'source', 'posted', 'retweeted_status_id', 'retweeted', 'in_reply_to_status_id']\n",
    "normal_to_drop = ['Tweet Id', 'Latitude', 'Longitude', 'Country', 'Profile picture', 'Tweet Url']\n",
    "troll_users = troll_users.drop(users_to_drop, axis=1)\n",
    "troll_tweets = troll_tweets.drop(tweets_to_drop, axis=1)\n",
    "normal_tweets = normal_tweets.drop(normal_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_new_cols = {'id': 'User Id', 'location': 'Location', 'name': 'User Name', \n",
    "                  'followers_count': 'Followers', 'lang': 'Language', 'screen_name': 'Nickname',\n",
    "                  'description': 'Bio', 'created_at': 'Account creation date', \n",
    "                  'friends_count': 'Following', 'listed_count': 'Listed'}\n",
    "tweets_new_cols = {'user_id': 'User Id', 'user_key': 'Nickname', 'created_str': 'Tweet date',\n",
    "                   'retweet_count': 'Retweets', 'favorite_count': 'Favorites', 'text': 'Tweet',\n",
    "                   'hashtags': 'Hashtags', 'expanded_urls': 'URLs', 'mentions': 'Mentions'}\n",
    "normal_new_cols = {'Tweet content': 'Tweet', 'Favs': 'Favorites', 'RTs': 'Retweets', \n",
    "                   'Place (as appears on Bio)': 'Location', 'Tweet language (ISO 639-1)': 'Language'}\n",
    "\n",
    "troll_users = troll_users.rename(columns=users_new_cols)\n",
    "troll_tweets = troll_tweets.rename(columns=tweets_new_cols)\n",
    "normal_tweets = normal_tweets.rename(columns=normal_new_cols)\n",
    "\n",
    "# Make times formats the same\n",
    "normal_tweets['Tweet date'] = normal_tweets['Date'] + ' ' + normal_tweets['Hour']\n",
    "normal_tweets = normal_tweets.drop(['Date', 'Hour'], axis=1)\n",
    "\n",
    "# Helps with merging\n",
    "troll_users = troll_users.drop(['User Id', 'Account creation date'], axis=1)\n",
    "troll_tweets = troll_tweets.drop('User Id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of each dataframe after cleaning :\n",
      "\n",
      "Troll users : ['Location' 'User Name' 'Followers' 'Language' 'Nickname' 'Bio' 'Following'\n",
      " 'Listed']\n",
      "\n",
      "Troll tweets : ['Nickname' 'Tweet date' 'Retweets' 'Favorites' 'Tweet' 'Hashtags' 'URLs'\n",
      " 'Mentions']\n",
      "\n",
      "Normal tweets : ['User Name' 'Nickname' 'Bio' 'Tweet' 'Favorites' 'Retweets' 'Location'\n",
      " 'Followers' 'Following' 'Listed' 'Language' 'Tweet date']\n"
     ]
    }
   ],
   "source": [
    "print('Columns of each dataframe after cleaning :\\n\\nTroll users : {}\\n\\nTroll tweets : {}\\n\\nNormal tweets : {}'\n",
    "      .format(troll_users.columns.values, troll_tweets.columns.values, normal_tweets.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling and merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the troll datasets\n",
    "trolls = pd.merge(left=troll_tweets, right=troll_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Tweet date</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>URLs</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Location</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Language</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Following</th>\n",
       "      <th>Listed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kathiemrr</td>\n",
       "      <td>2017-02-27 14:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#ThingsDoneByMistake kissing auntie in the lips</td>\n",
       "      <td>[\"ThingsDoneByMistake\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Imperfection is beauty, madness is genius and ...</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kathiemrr</td>\n",
       "      <td>2016-11-28 16:17:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jadedsweetangel: #ToDoListBeforeChristmas ...</td>\n",
       "      <td>[\"ToDoListBeforeChristmas\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2970.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Imperfection is beauty, madness is genius and ...</td>\n",
       "      <td>3157.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Nickname           Tweet date  Retweets  Favorites  \\\n",
       "0  kathiemrr  2017-02-27 14:54:00       NaN        NaN   \n",
       "1  kathiemrr  2016-11-28 16:17:36       NaN        NaN   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0    #ThingsDoneByMistake kissing auntie in the lips   \n",
       "1  RT @jadedsweetangel: #ToDoListBeforeChristmas ...   \n",
       "\n",
       "                      Hashtags URLs Mentions Location User Name  Followers  \\\n",
       "0      [\"ThingsDoneByMistake\"]   []       []  Atlanta    Kathie     2970.0   \n",
       "1  [\"ToDoListBeforeChristmas\"]   []       []  Atlanta    Kathie     2970.0   \n",
       "\n",
       "  Language                                                Bio  Following  \\\n",
       "0       en  Imperfection is beauty, madness is genius and ...     3157.0   \n",
       "1       en  Imperfection is beauty, madness is genius and ...     3157.0   \n",
       "\n",
       "   Listed  \n",
       "0    22.0  \n",
       "1    22.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trolls.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Name</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Location</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Listed</th>\n",
       "      <th>Language</th>\n",
       "      <th>Tweet date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Schulhoff</td>\n",
       "      <td>BillSchulhoff</td>\n",
       "      <td>Husband,Dad,GrandDad,Ordained Minister, Umpire...</td>\n",
       "      <td>Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East Patchogue, NY</td>\n",
       "      <td>386.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>en</td>\n",
       "      <td>2016-04-16 12:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniele Polis</td>\n",
       "      <td>danipolis</td>\n",
       "      <td>Viagens, geek, moda, batons laranja, cabelos c...</td>\n",
       "      <td>Pausa pro café antes de embarcar no próximo vô...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Grapevine, TX</td>\n",
       "      <td>812.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>pt</td>\n",
       "      <td>2016-04-16 12:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User Name       Nickname  \\\n",
       "0  Bill Schulhoff  BillSchulhoff   \n",
       "1   Daniele Polis      danipolis   \n",
       "\n",
       "                                                 Bio  \\\n",
       "0  Husband,Dad,GrandDad,Ordained Minister, Umpire...   \n",
       "1  Viagens, geek, moda, batons laranja, cabelos c...   \n",
       "\n",
       "                                               Tweet  Favorites  Retweets  \\\n",
       "0  Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...        NaN       NaN   \n",
       "1  Pausa pro café antes de embarcar no próximo vô...        NaN       NaN   \n",
       "\n",
       "             Location  Followers  Following  Listed Language        Tweet date  \n",
       "0  East Patchogue, NY      386.0      705.0    24.0       en  2016-04-16 12:44  \n",
       "1       Grapevine, TX      812.0      647.0    16.0       pt  2016-04-16 12:44  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_tweets.iloc[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put labels\n",
    "trolls['y'] = pd.Series([1 for x in range(len(trolls))], index=trolls.index)\n",
    "normal_tweets['y'] = pd.Series([0 for x in range(len(normal_tweets))], index=normal_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408271, 16)\n"
     ]
    }
   ],
   "source": [
    "# Merge normal and troll data \n",
    "tweets = trolls.append(normal_tweets, ignore_index=True)\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random rows we will use for visualization\n",
    "rows = [random.randint(0, len(tweets)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bio</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Language</th>\n",
       "      <th>Listed</th>\n",
       "      <th>Location</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet date</th>\n",
       "      <th>URLs</th>\n",
       "      <th>User Name</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82467</th>\n",
       "      <td>Black is my religion. If you buy me lipstick w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>768.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>12.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>[]</td>\n",
       "      <td>brianaregland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @FeministaJones: I'm tired of it\\r\\nWeary a...</td>\n",
       "      <td>2016-12-07 09:59:36</td>\n",
       "      <td>[]</td>\n",
       "      <td>Briana Ragland</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221959</th>\n",
       "      <td>To find the up to date weather conditions im m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Kentucky, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AppaloosaGuy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Trump in 2016 Temp:63.0°F Wind:0.0mph Pressur...</td>\n",
       "      <td>2016-04-16 03:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carl King</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MstrGoodmorning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Pru in blue and gold, the Boston's officia...</td>\n",
       "      <td>2016-04-16 01:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr. Goodmorning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72778</th>\n",
       "      <td>if you don't work hard to achieve your dreams,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24344.0</td>\n",
       "      <td>21953.0</td>\n",
       "      <td>[\"ThingsNotTaughtAtSchool\"]</td>\n",
       "      <td>en</td>\n",
       "      <td>113.0</td>\n",
       "      <td>Pittsburgh, US</td>\n",
       "      <td>[]</td>\n",
       "      <td>giselleevns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @MrMoomjean: Grammer #ThingsNotTaughtAtSchool</td>\n",
       "      <td>2016-10-12 14:15:28</td>\n",
       "      <td>[]</td>\n",
       "      <td>Giselle Evans</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Bio  Favorites  \\\n",
       "82467   Black is my religion. If you buy me lipstick w...        NaN   \n",
       "221959  To find the up to date weather conditions im m...        NaN   \n",
       "245669                                                NaN        NaN   \n",
       "72778   if you don't work hard to achieve your dreams,...        NaN   \n",
       "\n",
       "        Followers  Following                     Hashtags Language  Listed  \\\n",
       "82467       768.0      415.0                           []       en    12.0   \n",
       "221959       83.0       17.0                          NaN       en     9.0   \n",
       "245669      175.0      250.0                          NaN       en    11.0   \n",
       "72778     24344.0    21953.0  [\"ThingsNotTaughtAtSchool\"]       en   113.0   \n",
       "\n",
       "              Location Mentions         Nickname  Retweets  \\\n",
       "82467              USA       []    brianaregland       NaN   \n",
       "221959   Kentucky, USA      NaN     AppaloosaGuy       NaN   \n",
       "245669      Boston, MA      NaN  MstrGoodmorning       NaN   \n",
       "72778   Pittsburgh, US       []      giselleevns       NaN   \n",
       "\n",
       "                                                    Tweet  \\\n",
       "82467   RT @FeministaJones: I'm tired of it\\r\\nWeary a...   \n",
       "221959  #Trump in 2016 Temp:63.0°F Wind:0.0mph Pressur...   \n",
       "245669  The Pru in blue and gold, the Boston's officia...   \n",
       "72778    RT @MrMoomjean: Grammer #ThingsNotTaughtAtSchool   \n",
       "\n",
       "                 Tweet date URLs        User Name  y  \n",
       "82467   2016-12-07 09:59:36   []   Briana Ragland  1  \n",
       "221959     2016-04-16 03:45  NaN        Carl King  0  \n",
       "245669     2016-04-16 01:28  NaN  Mr. Goodmorning  0  \n",
       "72778   2016-10-12 14:15:28   []    Giselle Evans  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_hashtag(string):\n",
    "    htags = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '#':\n",
    "            htags.append(word[1:])\n",
    "    splits= []\n",
    "    # Puts space before capital letters and numbers\n",
    "    for h in htags:\n",
    "        splits.append(re.sub(r\"([A-Z0-9])\", r\" \\1\", h)[1:])\n",
    "    return splits\n",
    "\n",
    "def extract_mentions(string):\n",
    "    men = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[0] == '@':\n",
    "            if word[-1] == ':':\n",
    "                men.append(word[1:-1])\n",
    "            else:\n",
    "                men.append(word[1:])\n",
    "    return men\n",
    "\n",
    "# Not implemented yet, takes way too long to run on ~400,000 tweets\n",
    "def extract_links(string):\n",
    "    links = []\n",
    "    string = str(string)\n",
    "    for word in string.split():\n",
    "        if word[:4] == 'http':\n",
    "            try:\n",
    "                link = urlopen(word).geturl()\n",
    "                links.append(link)\n",
    "            except:\n",
    "                pass\n",
    "    return links\n",
    "\n",
    "def extract_website(string):\n",
    "    string = str(string)\n",
    "    start = string.find('//') + 2\n",
    "    if start > 1:\n",
    "        end = string[start:].find('/') + start\n",
    "        return string[start:end]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330                  twitter.com\n",
       "655              www.newsmax.com\n",
       "696                wikileaks.org\n",
       "1306                 twitter.com\n",
       "1504                 twitter.com\n",
       "1574                    paper.li\n",
       "1621                     vine.co\n",
       "2002                 twitter.com\n",
       "2833                 twitter.com\n",
       "3340                 twitter.com\n",
       "3461                 twitter.com\n",
       "3758                       ln.is\n",
       "3874                       ln.is\n",
       "4028            proudemocrat.com\n",
       "4095                 twitter.com\n",
       "4675                 twitter.com\n",
       "5172           cards.twitter.com\n",
       "5204                   on.cc.com\n",
       "5634                 twitter.com\n",
       "5719                 twitter.com\n",
       "7333                 twitter.com\n",
       "7337             blacktolive.org\n",
       "7342                      bit.ly\n",
       "7343                      bit.ly\n",
       "7344                 twitter.com\n",
       "7345                    youtu.be\n",
       "7351                      bit.ly\n",
       "7353                      bit.ly\n",
       "7356                 twitter.com\n",
       "7357           www.lifezette.com\n",
       "                   ...          \n",
       "203347                to.welt.de\n",
       "203348    www.ruhrnachrichten.de\n",
       "203349                    goo.gl\n",
       "203350                    goo.gl\n",
       "203352               www.faz.net\n",
       "203353          www.kn-online.de\n",
       "203355               www.welt.de\n",
       "203364         www.tagesschau.de\n",
       "203369          www.politico.com\n",
       "203370               www.cnn.com\n",
       "203371          www.politico.com\n",
       "203372    www.huffingtonpost.com\n",
       "203373                 riafan.ru\n",
       "203380                    bit.ly\n",
       "203381                   vine.co\n",
       "203386              news.sky.com\n",
       "203387        www.wonderzine.com\n",
       "203394         www.tagesschau.de\n",
       "203400               metro.co.uk\n",
       "203404                   dlvr.it\n",
       "203405                   usat.ly\n",
       "203421                   vine.co\n",
       "203423                  youtu.be\n",
       "203424                  youtu.be\n",
       "203425                  youtu.be\n",
       "203430         www.georeview.org\n",
       "203431         www.tagesschau.de\n",
       "203433                   faz.net\n",
       "203441               twitter.com\n",
       "203448                to.welt.de\n",
       "Name: URLs, Length: 25314, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweets['URLs'].apply(extract_website)\n",
    "test[test != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f18a653eb238>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrolls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'URLs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alexander\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "trolls[(trolls['test'].str.len() != 0)][['Tweet', 'URLs', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Hashtags'] = tweets['Tweet'].apply(extract_hashtag)\n",
    "tweets['Mentions'] = tweets['Tweet'].apply(extract_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    string = str(string)\n",
    "    string = string.split()\n",
    "    to_remove = []\n",
    "    for word in string:\n",
    "        if word[0] == '#' or word[0] == '@' or word == 'RT':\n",
    "            to_remove.append(word)\n",
    "    for word in to_remove:\n",
    "        string.remove(word)\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['Tweet'] = tweets['Tweet'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[rows, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace NaNs with 0s where appropriate ()\n",
    "\n",
    "#replace NaNs with empty list where appropriate (mentions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for NLP : One hot encoding and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preliminary data reformatting\n",
    "#replace language with one-hot encoded form\n",
    "#reformat date (?)\n",
    "#probably won't use: bio, listed, location, nickname, username, URLs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove newlines for one-hot encoding\n",
    "tweets_1_hot = tweets.copy()\n",
    "tweets_1_hot['Tweet'].str.replace('\\n', ' ', case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create 1 hot dataframe :\n",
    "https://stackoverflow.com/questions/18889588/create-dummies-from-column-with-multiple-values-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create word2vec dataframe\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different methods for classification : SVM and K Nearsest Neighbors \n",
    "\n",
    "NOTE: The gridsearch objects may require parameters to reduce memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create SVM\n",
    "#initialize classifier objects for the 1-hot and word2vec encodings\n",
    "svc_1_hot = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "svc_word2vec = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#penalty parameter\n",
    "C_list =[10**(1-i) for i in range(6)]\n",
    "#kernel function - poly:polynomial\n",
    "kernel_list =['linear','rbf','sigmoid','poly']\n",
    "param_dic_SVM = {'C':C_list, 'kernel':kernel_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_svm_1 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train SVM using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_svm_2 = GridSearchCV(estimator, param_dict_SVM, cv=5, refit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create KNN\n",
    "\n",
    "knn_one_hot = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "knn_word2vec = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "#initialize parameters\n",
    "\n",
    "#number of nearest neighbors to consider: 3-10\n",
    "n_neighbors_list = [i+3 for i in range(8)]\n",
    "#weighting of neighbors, whether they all get an equal vote or are weighted by distance\n",
    "weights_list = ['uniform','distance']\n",
    "param_dic_knn = {'n_neighbors':n_neighbors_list,'weights':weights_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using GridSearch on one-hot\n",
    "save results in list\n",
    "\n",
    "clf_knn_1 = GridSearchCV(knn_one_hot, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train KNN using Gridsearch on w2v\n",
    "save results in list\n",
    "\n",
    "clf_knn_2 = GridSearchCV(knn_word2vec, param_dict_knn, cv=5, refit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plot and present results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
